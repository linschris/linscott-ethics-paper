@article{Fuchs2018,
	title = {The {Dangers} of {Human}-{Like} {Bias} in {Machine}-{Learning} {Algorithms}},
	volume = {2},
	url = {https://scholarsmine.mst.edu/peer2peer/vol2/iss1/1},
	number = {1},
	journal = {Missouri S\&T’s Peer to Peer},
	author = {Fuchs, Daniel},
	month = may,
	year = {2018},
}
@article {C3Clustering,
   author = {C3AI},
   title = {Clustering},
   url = {https://c3.ai/glossary/data-science/clustering/},
}
@article{Yu2020,
	title = {One {Algorithm} {May} {Not} {Fit} {All}: {How} {Selection} {Bias} {Affects} {Machine}                    {Learning} {Performance}},
	volume = {40},
	issn = {0271-5333},
	shorttitle = {One {Algorithm} {May} {Not} {Fit} {All}},
	url = {https://pubs.rsna.org/doi/full/10.1148/rg.2020200040},
	doi = {10.1148/rg.2020200040},
	abstract = {Machine learning (ML) algorithms have demonstrated high diagnostic accuracy in identifying and categorizing disease on radiologic images. Despite the results of initial research studies that report ML algorithm diagnostic accuracy similar to or exceeding that of radiologists, the results are less impressive when the algorithms are installed at new hospitals and are presented with new images. This phenomenon is potentially the result of selection bias in the data that were used to develop the ML algorithm. Selection bias has long been described by clinical epidemiologists as a key consideration when designing a clinical research study, but this concept has largely been unaddressed in the medical imaging ML literature. The authors discuss the importance of selection bias and its relevance to ML algorithm development to prepare the radiologist to critically evaluate ML literature for potential selection bias and understand how it might affect the applicability of ML algorithms in real clinical environments. ©RSNA, 2020},
	number = {7},
	urldate = {2022-04-06},
	journal = {RadioGraphics},
	author = {Yu, Alice                        C. and Eng, John},
	month = nov,
	year = {2020},
	note = {Publisher: Radiological Society of North America},
	pages = {1932--1937},
	file = {Full Text PDF:/Users/lchris/Zotero/storage/YMQSH2IR/Yu and Eng - 2020 - One Algorithm May Not Fit All How Selection Bias .pdf:application/pdf},
}
@misc{Warrick2020,
	title = {{YouTube}-{8M} {Dataset}},
	url = {https://medium.com/google-cloud/youtube-8m-dataset-c2ee9c79d136},
	abstract = {YouTube-8M is a project that was developed by Google AI/Research in 2016 to drive innovations and advancement in computer vision…},
	language = {en},
	urldate = {2022-04-16},
	journal = {Google Cloud - Community},
	author = {Warrick},
	month = may,
	year = {2020},
	file = {Snapshot:/Users/lchris/Zotero/storage/UM69XR6W/youtube-8m-dataset-c2ee9c79d136.html:text/html},
}
@misc{googleYT8M,
	title = {{YouTube}-{8M}: {A} {Large} and {Diverse} {Labeled} {Video} {Dataset} for {Video} {Understanding} {Research}},
	url = {https://research.google.com/youtube8m/},
	urldate = {2022-04-16},
	file = {YouTube-8M\: A Large and Diverse Labeled Video Dataset for Video Understanding Research:/Users/lchris/Zotero/storage/FPJ9NISZ/youtube8m.html:text/html},
}
@misc{fairness2017,
	title = {Machines are getting schooled on fairness},
	url = {https://www.sciencenews.org/article/machines-are-getting-schooled-fairness},
	abstract = {Machine-learning programs are introducing biases that may harm job seekers, loan applicants and more.},
	language = {en-US},
	urldate = {2022-04-15},
	journal = {Science News},
	month = sep,
	year = {2017},
	file = {Snapshot:/Users/lchris/Zotero/storage/S7XRXRAK/machines-are-getting-schooled-fairness.html:text/html},
}
@article{Rajkomar2018,
	title = {Ensuring {Fairness} in {Machine} {Learning} to {Advance} {Health} {Equity}},
	volume = {169},
	issn = {0003-4819},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6594166/},
	doi = {10.7326/M18-1990},
	abstract = {Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past—called protected groups—are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice—specifically those that ensure equality in patient outcomes, performance, and resource allocation—and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.},
	number = {12},
	urldate = {2022-04-15},
	journal = {Annals of internal medicine},
	author = {Rajkomar, Alvin and Hardt, Michaela and Howell, Michael D. and Corrado, Greg and Chin, Marshall H.},
	month = dec,
	year = {2018},
	pmid = {30508424},
	pmcid = {PMC6594166},
	pages = {866--872},
	file = {PubMed Central Full Text PDF:/Users/lchris/Zotero/storage/APEYD9CU/Rajkomar et al. - 2018 - Ensuring Fairness in Machine Learning to Advance H.pdf:application/pdf},
}
@misc{Zippia2021,
	title = {Content {Creator} {Demographics} and {Statistics} [2022]: {Number} {Of} {Content} {Creators} {In} {The} {US}},
	shorttitle = {Content {Creator} {Demographics} and {Statistics} [2022]},
	url = {https://www.zippia.com/content-creator-jobs/demographics/},
	abstract = {What are the US demographics for Content Creators? Learn more about 2022 demographics based on factors such as age, race, sex, salary and location.},
	language = {en-US},
	urldate = {2022-04-15},
	month = jan,
	year = {2021},
	file = {Snapshot:/Users/lchris/Zotero/storage/5PKPX8ML/demographics.html:text/html},
}
@article{Hellstrom2020,
	title = {Bias in {Machine} {Learning} -- {What} is it {Good} for?},
	url = {http://arxiv.org/abs/2004.00686},
	abstract = {In public media as well as in scientific publications, the term {\textbackslash}emph\{bias\} is used in conjunction with machine learning in many different contexts, and with many different meanings. This paper proposes a taxonomy of these different meanings, terminology, and definitions by surveying the, primarily scientific, literature on machine learning. In some cases, we suggest extensions and modifications to promote a clear terminology and completeness. The survey is followed by an analysis and discussion on how different types of biases are connected and depend on each other. We conclude that there is a complex relation between bias occurring in the machine learning pipeline that leads to a model, and the eventual bias of the model (which is typically related to social discrimination). The former bias may or may not influence the latter, in a sometimes bad, and sometime good way.},
	urldate = {2022-04-16},
	journal = {arXiv:2004.00686 [cs]},
	author = {Hellström, Thomas and Dignum, Virginia and Bensch, Suna},
	month = sep,
	year = {2020},
	note = {arXiv: 2004.00686},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/lchris/Zotero/storage/9R38RJYA/Hellström et al. - 2020 - Bias in Machine Learning -- What is it Good for.pdf:application/pdf;arXiv.org Snapshot:/Users/lchris/Zotero/storage/UDTYA9XB/2004.html:text/html},
}
@article{Hu2019,
	title = {Should {Researchers} {Be} {Allowed} to {Use} {YouTube} {Videos} and {Tweets}?},
	issn = {1091-2339},
	url = {https://slate.com/technology/2019/06/youtube-twitter-irb-human-subjects-research-social-media-mining.html},
	abstract = {A new paper used YouTubers’ voices to guess what they looked like. We’re going to see more of this.},
	language = {en-US},
	urldate = {2022-04-16},
	journal = {Slate},
	author = {Hu, Jane C.},
	month = jun,
	year = {2019},
	keywords = {ethics, privacy, research, social-media, youtube},
	file = {Snapshot:/Users/lchris/Zotero/storage/2WURLEE9/youtube-twitter-irb-human-subjects-research-social-media-mining.html:text/html},
}
@misc{CrashCourseABias,
	title = {Algorithmic {Bias} and {Fairness}: {Crash} {Course} {AI} \#18},
	shorttitle = {Algorithmic {Bias} and {Fairness}},
	url = {https://www.youtube.com/watch?v=gV0_raKR2UQ},
	abstract = {Check out my collab with "Above the Noise" about Deepfakes: https://www.youtube.com/watch?v=Ro8b6...
Today, we're going to talk about five common types of algorithmic bias we should pay attention to: data that reflects existing biases, unbalanced classes in training data, data that doesn't capture the right value, data that is amplified by feedback loops, and malicious data. Now bias itself isn't necessarily a terrible thing, our brains often use it to take shortcuts by finding patterns, but bias can become a problem if we don't acknowledge exceptions to patterns or if we allow it to discriminate.},
	urldate = {2022-04-16},
	author = {{CrashCourse}},
	month = dec,
	year = {2019},
}
@misc{HootSuite2022,
	title = {23 {YouTube} {Stats} {That} {Matter} to {Marketers} in 2022},
	url = {https://blog.hootsuite.com/youtube-stats-marketers/},
	abstract = {Here are the most important YouTube statistics marketers should know for 2022. Some of them are surprising!},
	language = {en-US},
	urldate = {2022-04-14},
	journal = {Social Media Marketing \& Management Dashboard},
	month = feb,
	year = {2022},
	file = {Snapshot:/Users/lchris/Zotero/storage/RE6DDTCL/youtube-stats-marketers.html:text/html},
}
@article {PrasadClustering,
   author = {Sunit Prasad},
   title = {Different Types of Clustering Methods and Applications},
   url = {https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/#sub1},
}
@misc{Muller2021,
	title = {Clickbait is {Unreasonably} {Effective}},
	url = {https://www.youtube.com/watch?v=S2xHZPH5Sng},
	abstract = {The title and thumbnail play a huge role in a video's success or failure.
Written by Derek Muller
Animation by Iván Tello
Filmed by Derek Muller and Emily Zhang
Additional video supplied by Getty Images 
Produced by Derek Muller, Emily Zhang and Petr Lebedev},
	urldate = {2022-04-16},
	author = {Derek Muller},
	month = aug,
	year = {2021},
}
@misc{GeeksForGeeks2019,
	title = {{ML} {\textbar} {Fuzzy} {Clustering}},
	url = {https://www.geeksforgeeks.org/ml-fuzzy-clustering/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-us},
	urldate = {2022-04-16},
	journal = {GeeksforGeeks},
	month = sep,
	year = {2019},
	note = {Section: Articles},
	file = {Snapshot:/Users/lchris/Zotero/storage/HYRJGSTH/ml-fuzzy-clustering.html:text/html},
}
@misc{YTAddingThumbnails,
	title = {Add video thumbnails on {YouTube} - {YouTube} {Help}},
	url = {https://support.google.com/youtube/answer/72431?hl=en},
	urldate = {2022-04-16},
	file = {Add video thumbnails on YouTube - YouTube Help:/Users/lchris/Zotero/storage/N69ABDVL/72431.html:text/html},
}
@misc{YTFairUse,
	title = {{YouTube} {Copyright} \& {Fair} {Use} {Policies} - {How} {YouTube} {Works}},
	url = {https://www.youtube.com/howyoutubeworks/policies/copyright/},
	abstract = {Everyone has access to YouTube’s Copyright Management Tools, which gives rights holders control of their copyrighted material on YouTube.},
	language = {en},
	urldate = {2022-04-16},
	journal = {YouTube Copyright \& Fair Use Policies - How YouTube Works},
	file = {Snapshot:/Users/lchris/Zotero/storage/F97HANIF/copyright.html:text/html},
}
@article{Wang2020,
	title = {Decorrelated {Clustering} with {Data} {Selection} {Bias}},
	url = {http://arxiv.org/abs/2006.15874},
	abstract = {Most of existing clustering algorithms are proposed without considering the selection bias in data. In many real applications, however, one cannot guarantee the data is unbiased. Selection bias might bring the unexpected correlation between features and ignoring those unexpected correlations will hurt the performance of clustering algorithms. Therefore, how to remove those unexpected correlations induced by selection bias is extremely important yet largely unexplored for clustering. In this paper, we propose a novel Decorrelation regularized K-Means algorithm (DCKM) for clustering with data selection bias. Specifically, the decorrelation regularizer aims to learn the global sample weights which are capable of balancing the sample distribution, so as to remove unexpected correlations among features. Meanwhile, the learned weights are combined with k-means, which makes the reweighted k-means cluster on the inherent data distribution without unexpected correlation influence. Moreover, we derive the updating rules to effectively infer the parameters in DCKM. Extensive experiments results on real world datasets well demonstrate that our DCKM algorithm achieves significant performance gains, indicating the necessity of removing unexpected feature correlations induced by selection bias when clustering.},
	urldate = {2022-04-16},
	journal = {arXiv:2006.15874 [cs, stat]},
	author = {Wang, Xiao and Fan, Shaohua and Kuang, Kun and Shi, Chuan and Liu, Jiawei and Wang, Bai},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.15874},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted by main track of IJCAI2020;SOLE copyright holder is IJCAI (international Joint Conferences on Artificial Intelligence), all rights reserved},
	file = {arXiv Fulltext PDF:/Users/lchris/Zotero/storage/F58HXMJT/Wang et al. - 2020 - Decorrelated Clustering with Data Selection Bias.pdf:application/pdf;arXiv.org Snapshot:/Users/lchris/Zotero/storage/53T3UXUK/2006.html:text/html},
}