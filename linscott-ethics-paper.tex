% !TEX TS-program = pdflatex
\documentclass[10pt,twocolumn]{article} 

% required packages for Oxy Comps style
\usepackage{oxycomps} % the main oxycomps style file
\usepackage{times} % use Times as the default font
\usepackage[style=numeric,sorting=nyt]{biblatex} % format the bibliography nicely 

\usepackage{amsfonts} % provides many math symbols/fonts
\usepackage{listings} % provides the lstlisting environment
\usepackage{amssymb} % provides many math symbols/fonts
\usepackage{graphicx} % allows insertion of grpahics
\usepackage{hyperref} % creates links within the page and to URLs
\usepackage{url} % formats URLs properly
\usepackage{verbatim} % provides the comment environment
\usepackage{xpatch} % used to patch \textcite

\bibliography{refs.bib}
\DeclareNameAlias{default}{last-first}

\xpatchbibmacro{textcite}
  {\printnames{labelname}}
  {\printnames{labelname} (\printfield{year})}
  {}
  {}

\pdfinfo{
    /Title (Image Search In Video Platforms With The Fuzzy C-Means Algorithm)
    /Author (Christopher Linscott)
}

\title{The Unavoidable and Unfixable Biases Within Machine Learning Algorithms}

\author{Christopher Linscott}
\affiliation{Occidental College}
\email{clinscott@oxy.edu}

\begin{document}

\maketitle

% Refer to rubic: https://docs.google.com/document/d/1oiXngqxh30ADXVPfOEnNuBNX1DGFmmExI6DoGZNdrs0/edit

\section {Introduction}

Creating a machine learning algorithm requires a balance between ethicality, accuracy, and efficiency. While addressing accuracy and efficiency require asking questions about one’s results and model, ethics require a deeper understanding about the implementation, data sources, and distribution of power behind it. Every step of the process including gathering and cleaning data, implementing and training the algorithm, and even determining what features the algorithm highlights, all play a huge role in how biased or unethical results may be. Because of unintentional and intentional data biases, as well as a lack of explainability behind my algorithm, overcoming these ethical barriers will not be possible.

\section {Project In Consideration}

For my comprehensive project, I will create a program which takes an image, of an object in real life or drawing, as input and returns related videos based on image similarities between the input image and any video’s thumbnail and individual video frames. The main problem of completing this task is determining how to group similar images together based on their visual characteristics, but a machine learning algorithm can utilize relationships in the data to solve this problem \cite{Fuchs2018}. In the context of video thumbnails and frames, these relationships will come from similarities in the background, color, and shape of given objects and people in these video frames and thumbnails. As the difference between images come from features or characteristics of the images, a clustering algorithm can “...[segment] a population into subgroups where members are more similar to each other… based on certain observed features” \cite{C3Clustering}. To solve this problem, the Fuzzy C-Means algorithm, a clustering algorithm, will be trained using a dataset containing YouTube video IDs, categories, and labeled characteristics of individual segments (portions of a video), frames, and thumbnails of a given video. With this design in mind, the main barrier to ethics comes initially in the first step of many machine learning algorithms: collecting and processing the data. 

\section {Unintentional Data Bias}

\subsection {Selection Bias}

Due to the limited amount of data and diversity surrounding video platforms and YouTube 8M’s dataset, unintentional biases such as selection, historical, and sample bias stop my machine learning algorithm from being ethical when first collecting and processing the data.
Selection bias, to be general, is the difference in characteristics between those selected for a study (training data) and those not (test / real world data) \cite{Yu2020}; it primarily occurs due to an underrepresentation of the population from the sample. YouTube’s 8M Dataset, which contains 6.1 million videos from 2017 and 2018 \cite{googleYT8M}, has much less data compared to the estimated 800 million on Youtube. While the dataset contains the data across the same amount of general categories (15), the distribution of data across these categories and subcategories is weaker, primarily having videos in the games or vehicle category \cite{googleYT8M}. While YouTube has the same number of categories, they are meant to encompass many of the videos together; the subcategories (and more niche categories) of YouTube beneath these general categories are not shown or displayed (more inferred and created by subcommunities). This combination of lack of data and diversity within the data contributes to poor generalization, or poor performance on the test set (real world) in comparison to the training set, due to the model overfitting to the training data and struggling to account for new characteristics from new input images \cite{Yu2020}. The main reason behind poor generalization in selection bias comes from unwanted correlations between features or characteristics in images; clustering on images with these correlated features lead to “spurious (fake) relationships” \cite{Yu2020}. A great example is working with an image dataset containing images of mainly dogs laying in grass. Upon the introduction of an image of a cat laying in grass, due to the high correlation between dogs and grass (implicitly in the data), the cat gets placed in the wrong cluster with the dogs. In a similar fashion, any unintentional and intentional biases of the YouTube 8M dataset will be exacerbated, by the model conforming to what it believes is the population (training data) and forming relationships about this data which don’t exist on all of YouTube. The subtypes of selection bias, historical and sample bias, can further describe the specific limitations and contribution effects of bias in our data and why our data cannot be ethical when used for machine learning.

\subsection {Historical Bias}

In addition to selection bias, the dataset exhibits historical bias, or bias created by “selective targeting over a period of time” \cite{Fuchs2018}. As noted in the previous paragraph, the dataset contains data from only 2017 to 2018 at the latest; this doesn’t represent the videos posted before and after on YouTube. Having outdated data means that the relevant videos returned by my algorithm will only come from this preselected range of time; AI implementations can have higher false positives for its predictions based on skewed perspectives. COMPAS wrongly predicted blacks to reoffend more often, and an AI implementation, using Oakland crime data from the year before, sent police to non-white neighborhoods where it believed more crimes would occur \cite{fairness2017}. Because these models learn from a population which could have “experience[d] human and structural biases in the past”, groups of videos (or people) are vulnerable to harm by incorrect prediction \cite{Rajkomar2018}. Not only does the model predict the most “relevant” videos based on this population, my model is likely to exhibit harm on other groups unintentionally by sending users to videos of non-minority groups, as a vast majority of content creators are still white \cite{Zippia2021}. A methodology to solve these biases may involve scraping data from the YouTube platform myself. However, parsing, storing, and training the machine algorithm overtime would take more time and resources than the scope of the project offers. As well as, even if the data is perfectly sampled and collected, a different form of historical bias may occur by the “world as it is or was [leading] a model to produce [unwanted] outcomes” \cite{Hellstrom2020} simply due to the high number of unknown dimensions and factors affecting data. Therefore, removing selection and historical bias requires the understanding of implicit, unknown variables and creating data without selection nor historical bias is too difficult to address.

\subsection {Sample Bias}

As a final subset of selection bias, due to the selection of data by YouTube, the YouTube dataset has sample bias and a favoring toward Western ideals. To reiterate, the videos from the dataset are primarily distributed in games, vehicles, and concerts with around 780,000 videos, whereas only 3000 videos are available about Indian cuisine and TV. The major advertising audience from YouTube is aged 25-34, male, and from India \cite{HootSuite2022}; despite this, there is a disparity in content surrounding India. Part of the reason behind the creation of the dataset was addressing the need for “large-scale, labeled datasets” and “[accelerating] research on large scale video understanding” \cite{Warrick2020}. As gaming and car content thrives on YouTube, this data may’ve been curated to not only appeal to Western developers, but to bring research on the largest sources of videos; in the process, this disfavors content from other backgrounds and lessens the diversity of the data. This selection of data affects the clustering algorithm in a similar way to sample bias, or bias due to nonrandom data. Like selection bias, sample bias will cause correlation between features; as non-popular videos (due to language or cultural barriers) are less likely picked (and have small groups), it’s less likely the clustering algorithm will create a separate cluster for them. In the process, the algorithm is less likely to group objects and people from lesser-known videos/backgrounds together, instead placing them in a larger cluster (from some other identifying feature) with less related videos. Additionally, the algorithm may choose to focus on a well-known feature on a lesser-known video as opposed to a lesser-known, identifying feature of the video or thumbnail. As a result, finding related videos for an object or person not recognized in the United States becomes more difficult (and may even require parsing through the relevant videos which defeats the purpose). Evidently, the unintentional biases present within my data contribute to the algorithm making incorrect relationships in the data and unethical decisions (clusters) based on its interpretation of the population.

\section {Intentional Data Bias}

\subsection {Design Decisions}

While unintentional biases may arise during the data collection process, intentional bias may arise during the design and implementation process, as a decision to prioritize solving the problem. In conjunction with the bias in the data, new decisions around input of my machine learning algorithm present an ethical issue. In order to train my algorithm to cluster images together, it needs raw images or well-described features to train on. While the YouTube 8M Dataset is valid and provides videos, categories, and features, it doesn’t provide raw image or video data to train my algorithm with. While the vocabulary (or possible values for features) describes in text what a given object is \cite{googleYT8M}, I can’t gather information about the actual components which make up the frame or thumbnail to find image similarities. Not only do the features describe a few, major components (without catching smaller, identifying components), but the labeled features have no necessity in the context of clustering (as grouping by textual labels doesn’t require machine learning). Therefore, I’ve decided to use a script which can grab the videos (from the video ids in the dataset) for me to train on. However, this will run into an ethical issue: breaking YouTube’s terms of service. While this is not illegal due to the Fair Act \cite{YTFairUse}, downloading YouTube videos can be considered unethical as I’m stealing others’ content (even copyrighted content) for my own personal use in research without asking for permission. As well as, using peoples’ faces and content in my publication may make people feel uncomfortable. Nick Sullivan’s face was utilized in a MIT’s student paper on computing appearance based on voices \cite{Hu2019}. While he didn’t physically sign away his rights to have his face used in research, as his content is publicly available on YouTube, it was completely legal (but arguably unethical). The topic of using publicly accessible data and social media posts is an ethical gray area, and it can be argued that utilizing these people’s faces, locations, objects, and backgrounds without their knowledge is unethical. Without this data, the algorithm will gather selection bias and my clustering results will suffer. As a developer, it’s incredibly difficult to bring concern to both having complete data and having ethical data at the same time.

\subsection {Image Manipulation}

In addition to the design process, intentional bias, arising from the decisions of users and creators, can show in the form of image manipulation in video thumbnails and frames.	
In the context of content creation, often videos will be edited for the purposes of drawing more attention or making the content more interesting to the viewer; a great source of clickbait comes from YouTube as a means to generate higher clickthrough rates and more revenue \cite{Muller2021}. As well as, thumbnails and video edits made to the frames are mainly made to manipulate the YouTube algorithm; they are not always made to showcase the underlying content. Therefore, as my algorithm clusters images together by their visual content, and not by any knowledge of content, this image manipulation (as we view frames) will show in the data as outliers, affected characteristics, or affected data points; these all have an inverse affect the algorithm’s ability to cluster related videos together. If an image is manipulated enough, the distinctive features of the image will register as different values (and data points) to our algorithm, causing it to be clustered amongst videos unrelated to it. A great example of this may be someone editing the grass in their video to look blue, therefore making the algorithm cluster it with other blue objects or water. Even in the case where thumbnails are not made by the users, YouTube will automatically generate a random thumbnail based on a random frame of the video \cite{YTAddingThumbnails}; this may end up backfiring in terms of providing meaningful content from the image. YouTube’s algorithm may pick a frame of the woodworker’s face as opposed to the saw he uses, despite the content of the video being about his woodworking. Image manipulation, amongst data and algorithmic bias, is another threat during deployment; not only does it contribute to data bias, but it acts as a vulnerability for the algorithm itself. Microsoft’s “Zo”, a deployed chatbot which published tweets based on previous chats, was exploited and attacked by “trolls”, causing the chatbot to publish insensitive tweets with derogatory terms \cite{CrashCourseABias}. If my algorithm gets exploited by new input or false bias, it’ll begin to make clusters of videos which are not accurate and are insensitive (e.g. showing pictures of violence upon an input image of an apple). In these cases, as the power of the algorithm is transferred to the users via this vulnerability, while the developer and algorithm did not intentionally act unethically, it can be said the developer’s responsibility is to account for these biases or malicious inputs. However, from the developer’s perspective, he’s limited in options; he can remove or filter abusive content, but this leads back to contributing to data bias (by unintentionally selecting a smaller sample).

\section {Transparency and Explainability}

As a final point, due to the lack of explainability behind the machine learning algorithm’s decisions, finding and fixing the sources of unethicality (bias) become too tedious and difficult to handle. While many machine learning algorithms, Fuzzy C-Means included, are mathematically explainable in how they converge or determine clusters, these explanations don’t describe why they decided on a specific set of clusters. For the Fuzzy C-Means algorithm, as the initial centers of centroids are picked at random and the placement of these initial centers affects the end result, there’s no way to backtrack mathematically. The algorithm can perform very well or very poorly, and the best method of overcoming this simply involves recomputing the clusters until the error (or distance between the cluster’s centroid and its corresponding distance) is minimized \cite{GeeksForGeeks2019}. As a consequence of this implementation, fixing a known source of bias and error, through fixes in the implementation, is difficult; telling the computer where the centers should be removes the machine learning aspect of this project.
As well as, if and when my algorithm acts unfairly, I cannot determine the source of its decision whether it’s due to the data or implementation of my algorithm. While I can reiterate and determine a relatively good set of clusters (using methods like the Elbow method to evaluate them \cite{PrasadClustering}), this doesn’t ensure optimal or unbiased results. This translates to deployment where the algorithm may perform horribly and under new data, return unexpected results without any sort of way to determine where it went wrong. The main solution to this problem is either removing the root cause, or removing the project from deployment immediately (like with Microsoft’s Zo); neither of these is sufficient for completing and deploying my project. Either I have to micromanage the algorithm (which defeats the purpose of it being machine learning), or I have to start from scratch with the data and algorithm (which already has presented problems).

\printbibliography
 
\end{document}
